{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8tinZOUlDER"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\">\n",
        "\n",
        "# FIT5196 Task 2 in Assessment 1\n",
        "    \n",
        "#### Student Name: Yehezkiel Efraim Darmadi, Yogi Sarumaha\n",
        "#### Student ID: 34078215, 34087672\n",
        "\n",
        "Date: 22 August 2024\n",
        "\n",
        "Environment: Python3\n",
        "\n",
        "### Libraries used:\n",
        "\n",
        "* **os** (for interacting with the operating system, included in Python 3 from Colab)\n",
        "* **pandas 1.1.0** (for working with dataframes, installed and imported)\n",
        "* **multiprocessing** (for performing processes on multiple cores, included in Python 3.6.9 package)\n",
        "* **itertools** (for performing operations on iterables, included in Python 3.x.x)\n",
        "* **nltk 3.5** (Natural Language Toolkit, installed and imported, for text processing tasks)\n",
        "  - **nltk.tokenize** (for tokenization, installed and imported)\n",
        "  - **nltk.stem** (for stemming the tokens, installed and imported)\n",
        "  - **nltk.probability** (for working with frequency distributions, included in the Natural Language Toolkit)\n",
        "  - **nltk.util** (for generating n-grams, included in the Natural Language Toolkit)\n",
        "* **re** (for defining and using regular expressions, included in Python 3.x.x)\n",
        "* **matplotlib.pyplot** (for generating visualizations and plots, installed and imported)\n",
        "* **collections** (specifically `defaultdict`, for creating dictionary-like collections with default values, included in Python 3.x.x)\n",
        "\n",
        "    </div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnnLnFnLlDEU"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "    \n",
        "## Table of Contents\n",
        "\n",
        "</div>\n",
        "\n",
        "[1. Introduction](#Intro) <br>\n",
        "[2. Importing Libraries](#libs) <br>\n",
        "[3. Examining Input File](#examine) <br>\n",
        "[4. Loading and Parsing Files](#load) <br>\n",
        "$\\;\\;\\;\\;$[4.1. Tokenization](#tokenize) <br>\n",
        "$\\;\\;\\;\\;$[4.2. Removing Context-Independent Stopwords](#context_independent) <br>\n",
        "$\\;\\;\\;\\;$[4.3. Filtering Tokens](#filtering_tokens) <br>\n",
        "$\\;\\;\\;\\;$[4.4. Stemming](#stemming) <br>\n",
        "$\\;\\;\\;\\;$[4.5. Removing Context-Dependent Stopwords](#context_dependent) <br>\n",
        "$\\;\\;\\;\\;$[4.6. Removing Rare Tokens](#rare_tokens) <br>\n",
        "$\\;\\;\\;\\;$[4.7. Generating Bigrams](#bigrams) <br>\n",
        "$\\;\\;\\;\\;$[4.8. Matrix Representation](#Matrix) <br>\n",
        "[5. Writing Output Files](#write) <br>\n",
        "$\\;\\;\\;\\;$[5.1. Vocabulary List](#write-vocab) <br>\n",
        "$\\;\\;\\;\\;$[5.2. Sparse Matrix](#write-sparseMat) <br>\n",
        "[6. Summary](#summary) <br>\n",
        "[7. References](#Ref) <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8mo6PPRlDEU"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "    \n",
        "## 1.  Introduction  <a class=\"anchor\" name=\"Intro\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewZrff73lDEV"
      },
      "source": [
        "This assessment concerns textual data, and the aim is to extract data, process it, and transform it into a proper format. The dataset provided is in the format of a PDF file containing Google Map reviews from businesses in California. The task involves pre-processing the text data to convert it into numerical representations suitable for downstream modeling tasks. Specifically, the assignment required us to extract review text from businesses with at least 70 text reviews, clean the data by removing context-independent and context-dependent stopwords, stem the tokens, and generate a vocabulary list. The processed text is then transformed into a sparse numerical representation and exported in a specified format to be used for the exploratory data analysis that we combine with the meta data later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSr_kwKclDEV"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "    \n",
        "## 2.  Importing Libraries  <a class=\"anchor\" name=\"libs\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acwZw2NklDEW"
      },
      "source": [
        "In this assessment, various Python packages were utilized to accomplish the required tasks, including but not limited to:\n",
        "\n",
        "* **os**: For interacting with the operating system, such as navigating directories and handling file operations.\n",
        "* **re**: For defining and using regular expressions, which are essential for pattern matching in strings.\n",
        "* **pandas**: To work with dataframes, providing powerful data manipulation capabilities.\n",
        "* **multiprocessing**: To perform processes on multiple cores, improving the performance of computationally intensive tasks.\n",
        "* **itertools**: For efficient looping constructs, specifically for performing operations on iterables.\n",
        "* **nltk**: The Natural Language Toolkit, used for various text processing tasks.\n",
        "  - **nltk.probability**: For working with probability distributions and frequency distributions.\n",
        "  - **nltk.tokenize**: For tokenizing text into words or phrases.\n",
        "  - **nltk.stem**: For stemming tokens to their root forms.\n",
        "  - **nltk.util**: For working with n-grams, which are contiguous sequences of tokens.\n",
        "* **matplotlib.pyplot**: For creating visualizations and plots.\n",
        "* **collections**: Specifically, `defaultdict` is used to handle dictionary-like collections that provide a default value for non-existent keys.\n",
        "\n",
        "Additional libraries may be used as needed throughout the assignment to handle specific tasks related to data processing and analysis."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langid"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgIfZZzBg12P",
        "outputId": "bf577b3e-078e-47a7-9da6-4d2be4f14a5f"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langid in /usr/local/lib/python3.10/dist-packages (1.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from langid) (1.26.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "qgmGWs8HlDEW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import langid\n",
        "import pandas as pd\n",
        "import multiprocessing\n",
        "from itertools import chain\n",
        "import nltk\n",
        "from nltk.probability import *\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.tokenize import MWETokenizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.util import ngrams\n",
        "from __future__ import division\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwNp0KnWlDEX"
      },
      "source": [
        "-------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "SA7fSJiRlDEY"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "    \n",
        "## 3.  Examining Input File <a class=\"anchor\" name=\"examine\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Connect the notebook to the Google Drive"
      ],
      "metadata": {
        "id": "vKhdfLU2PkOw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPCuEl8smTHW"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CJDLDI6lDEY"
      },
      "source": [
        "Let's examine what is the content of the file. For this purpose, we will first load the file and inspect the format. By doing so,  we can determine the necessary steps for extracting the relevant information required for further processing."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### this need to be changed (for the tutor)\n",
        "input_files_dir = \"/content/drive/MyDrive/Data Wrangling/assignment 1/Task 1\""
      ],
      "metadata": {
        "id": "VMt2y4Tgg4YV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyLbqkRxmCEZ"
      },
      "outputs": [],
      "source": [
        "df_csv = pd.read_csv(input_files_dir + \"/task1_111.csv\")\n",
        "df_json = pd.read_json(input_files_dir + \"/task1_111.json\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_csv.head()"
      ],
      "metadata": {
        "id": "p7RILXIInk5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_json.head()"
      ],
      "metadata": {
        "id": "FDMEt-ymnqPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpDVyW4YlDEZ"
      },
      "source": [
        "It is noticed that the df_csv DataFrame appears to contain summary statistics or metadata related to reviews, such as gmap_id, review_count, review_text_count, and response_count. In contrast, the df_json DataFrame seems to be more detailed, containing nested data, such as reviews, earliest_review_date, and latest_review_date. Each column in df_json appears to correspond to a unique gmap_id, with each cell containing detailed review information, possibly including user IDs and timestamps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "ENnHWjoXlDEc"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "    \n",
        "## 4.  Loading and Parsing File <a class=\"anchor\" name=\"load\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9esGMx8lDEc"
      },
      "source": [
        "In this section, we are going to filter the gmap_id by the number of review_text_count. We want to analyse the gmap_ids that have at least 70 text reviews.\n",
        "\n",
        "For the first part of this section, we are going to deal with the dataframes and then transform the dataframe into dictionary to be pre-processed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsnRR2c4lDEc"
      },
      "outputs": [],
      "source": [
        "# filter the df_csv file\n",
        "df_csv_filtered = df_csv[df_csv[\"review_text_count\"] >= 70]\n",
        "\n",
        "# Extract the gmap_ids as a list\n",
        "gmap_ids_to_filter = df_csv_filtered[\"gmap_id\"].tolist()\n",
        "\n",
        "# Use the list to select columns from df_json\n",
        "df_json_filtered = df_json.loc[:, gmap_ids_to_filter]\n",
        "df_json_filtered.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see one of the example of the review."
      ],
      "metadata": {
        "id": "AslqCYj6t8Ab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_json_filtered[\"0x14e4bcd95f3c0451:0x7ccf04478a4d59af\"][\"reviews\"][0]"
      ],
      "metadata": {
        "id": "nCe1UVEWsZZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we want to analyse the google reviews for each of the business, it would make more sense if it combine the review_text into one per gmap_id. So, our goal is to concatenate the \"review_text\" for each use_id into a single review_text for each gmap_id. We can directly store it into a dictionary."
      ],
      "metadata": {
        "id": "Gh0frZFjuBfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "review_list = []\n",
        "\n",
        "gmap_id_list = df_csv_filtered[\"gmap_id\"].tolist()\n",
        "\n",
        "# looping each of the gmap_id\n",
        "for index, gmap_id in enumerate(gmap_id_list):\n",
        "  review_combine = \"\"\n",
        "  for review in df_json_filtered[gmap_id].loc[\"reviews\"]:\n",
        "    review_combine += review[\"review_text\"] + \" \"\n",
        "  review_list.append(review_combine)\n",
        "\n",
        "# make it into df\n",
        "df_review = pd.DataFrame(\n",
        "    {\n",
        "        \"gmap_id\": gmap_id_list,\n",
        "        \"review\": review_list\n",
        "    }\n",
        ")\n",
        "\n",
        "# transform it into dict\n",
        "gmap_id_review = dict(\n",
        "    zip(df_review[\"gmap_id\"].tolist(), df_review[\"review\"].tolist())\n",
        ")\n",
        "\n",
        "# let's see one gmap_id\n",
        "gmap_id_review[\"0x14e4bcd95f3c0451:0x7ccf04478a4d59af\"]"
      ],
      "metadata": {
        "id": "as724GkCuiif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above operation results in a dictionary with PID representing gmap_id and a single string for all reviews of the day concatenated to each other.\n",
        "\n",
        "Below are the steps to preprocessed the review text:\n",
        "1. Tokenization\n",
        "\n",
        "2. Removing Context-Independent Stopwords\n",
        "\n",
        "3. Filtering Tokens\n",
        "\n",
        "4. Stemming\n",
        "\n",
        "5. Removing Context-Dependent Stopwords\n",
        "\n",
        "6. Removing Rare Tokens\n",
        "\n",
        "7. Generating Bigrams\n",
        "\n",
        "8. Matrix Representation"
      ],
      "metadata": {
        "id": "pfEf1GG9woqd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "VIfQCD1VlDEe"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "    \n",
        "### 4.1. Tokenization <a class=\"anchor\" name=\"tokenize\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gchByyjolDEf"
      },
      "source": [
        "Tokenization is a principal step in text processing and producing unigrams. It is crucial because it breaks down the continuous stream of text into manageable units, or tokens, which are the basic building blocks for text analysis. By converting the text into tokens, we can more easily analyze the frequency, context, and relationships between words. This process also facilitates subsequent steps such as stopword removal, stemming, and the creation of numerical representations for modeling. The resulting tokens from our regex-based approach will ensure that we focus on meaningful words, laying a solid foundation for the remainder of the text processing workflow.\n",
        "\n",
        "In this section, we are going to tokenize the text using the following regex which is required by the assigment, the regex term is '[a-zA-Z]+'. This regular expression is designed to capture sequences of alphabetic characters, effectively isolating words from the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8zT4N0RlDEf"
      },
      "outputs": [],
      "source": [
        "# define the tokenizer\n",
        "tokenizer = RegexpTokenizer(r\"[a-zA-Z]+\")\n",
        "\n",
        "def tokenize_data(gmap_id):\n",
        "    \"\"\"\n",
        "    Tokenizes the review text associated with a given Google Map ID.\n",
        "\n",
        "    The function retrieves the review text corresponding to the provided\n",
        "    `gmap_id`, then tokenizes the text using a regular expression tokenizer\n",
        "    that extracts only alphabetic words (ignoring numbers, punctuation, etc.).\n",
        "    The result is returned as a tuple containing the `gmap_id` and the list of tokens.\n",
        "\n",
        "    Args:\n",
        "        gmap_id (str): The Google Map ID for which the review text should be tokenized.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the `gmap_id` and a list of tokenized words from the review.\n",
        "    \"\"\"\n",
        "    review = gmap_id_review[gmap_id]\n",
        "    tokenised_review = tokenizer.tokenize(review)\n",
        "    return gmap_id, tokenised_review\n",
        "\n",
        "gmap_id_token = dict(tokenize_data(gmap_id) for gmap_id in gmap_id_review.keys())\n",
        "\n",
        "#create copy for bigrams section\n",
        "gmap_id_bigram = gmap_id_token.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqZos1q6lDEf"
      },
      "source": [
        "Let's see one sample gmap_id below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPBNTTq6lDEg"
      },
      "outputs": [],
      "source": [
        "count = 0\n",
        "for key, value in gmap_id_token.items():\n",
        "    print(f\"{key}: {value[:10]}\")\n",
        "    count += 1\n",
        "    if count == 5:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVqFfwwMlDEg"
      },
      "source": [
        "At this stage, all reviews for each gmap_id are tokenized and are stored as a value in the new dictionary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NglwwiJRnPZd"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "    \n",
        "### 4.2. Removing Context-Independent Stopwords <a class=\"anchor\" name=\"context_independent\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at the values in the sample gmap_id that we have printed in the previous section. It can be seen that there are lots of unusable stopwords, such as 'i', 'a', etc. First let's import the stop words from the google drive."
      ],
      "metadata": {
        "id": "vGjGQ6CQ3G-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######## needs to be changed by the tutor\n",
        "stop_word_dir = \"/content/drive/Shareddrives/FIT5196_S2_2024/GroupAssessment1\"\n",
        "with open(stop_word_dir + '/stopwords_en.txt', 'r') as file:\n",
        "    # previously context_independent_stopwords\n",
        "    stop_words = set(file.read().splitlines())\n",
        "\n",
        "for i in range(0, 30, 10):\n",
        "    print(list(stop_words)[i:i+10])"
      ],
      "metadata": {
        "id": "czZz1iD03Uln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can be seen that the stopwords have not beet stemmed or lemmatized. Thus, it is preferable to actually remove the stopwords before we do any stemming.\n",
        "\n",
        "It can also be seen that there is \"i'll\" word in the stopwords while our regex is specified to capture sequences of alphabetic characters. Like our sample tokens in the the previous section, it can be seen there is a token 'don' and 't', which indicate the word \"don't\". This will be a problem later on that we need to address."
      ],
      "metadata": {
        "id": "xcorePxX4dN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# removing stopwords\n",
        "for key, value in gmap_id_token.items():\n",
        "    gmap_id_token[key] = [word for word in value if word not in stop_words]"
      ],
      "metadata": {
        "id": "vVMB46455yE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's see the sample result\n",
        "for i in range(0, 100, 10):\n",
        "    print(gmap_id_token[\"0x14e4bcd95f3c0451:0x7ccf04478a4d59af\"][i:i+10])"
      ],
      "metadata": {
        "id": "dOwOROUF6G-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The \"don\" is still lingering in the dictionary, we are going to deal with this later after stemming and removing tokens. However, stemming with the combination of \"removing context-dependent stopwords\" will remove these tokens. The explanation will be explain in the later sections."
      ],
      "metadata": {
        "id": "WsfXVlz264_f"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzINPkyb17QY"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "    \n",
        "### 4.3. Filtering Tokens <a class=\"anchor\" name=\"filtering_tokens\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filtering out unusable tokens is crucial, and in this context, we assume that tokens with fewer than three letters are not significant. Short tokens like “la” and “pa,” as seen in the previous section, may not contribute meaningfully to the analysis and could introduce noise, potentially skewing results and reducing accuracy.\n",
        "\n",
        "By removing these short tokens, we can focus on more substantial words that carry meaningful information, enhancing the quality and relevance of the data. However, a more thorough analysis is necessary to determine which words should be removed to optimize the process."
      ],
      "metadata": {
        "id": "91n4GKw770Xl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (Monash University, 2024)\n",
        "words = list(chain.from_iterable(gmap_id_token.values()))\n",
        "freq_dist_1 = FreqDist(words)\n",
        "filtered_freq_dist = FreqDist({word: freq for word, freq in freq_dist_1.items() if len(word) < 3})\n",
        "\n",
        "print(f\"The number of tokens that have less than 3 is: {len(filtered_freq_dist)}\")\n",
        "filtered_freq_dist.keys()"
      ],
      "metadata": {
        "id": "a9vwpuGn86x-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "less_common_words = filtered_freq_dist.hapaxes()\n",
        "\n",
        "less_common_words[:10]"
      ],
      "metadata": {
        "id": "8r1It2pvAKbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try to search what are the text that has those values."
      ],
      "metadata": {
        "id": "5Q-CiYwf-jeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.Text(words).concordance('pa')"
      ],
      "metadata": {
        "id": "HENreWHRJwT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.Text(words).concordance('dr')"
      ],
      "metadata": {
        "id": "ndVIlshxHZjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.Text(words).concordance('perlman')"
      ],
      "metadata": {
        "id": "3YCvAFeqHt2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.Text(words).concordance('ll')"
      ],
      "metadata": {
        "id": "wqrWc4WVHMOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.Text(words).concordance('tv')"
      ],
      "metadata": {
        "id": "riy17aQCHXBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.Text(words).concordance('television')"
      ],
      "metadata": {
        "id": "t8pPvCRq-n0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the values, most of the tokens that have length less than 3 are stopwords, that previously we have not addressed due to the regex limitation.\n",
        "\n",
        "Another thing worth mentioning is that some of the most rare tokens that have length less than 3, those tokens are very rare and thus will not have significant impact to the analysis. Thus, it needs to be removed as well.\n",
        "\n",
        "The last thing is that, some tokens up there like \"dr\" is an abbreviation for \"Doctor\", which is used in the context of calling a doctor name such as \"Dr. Lin\". The name \"Lin\" will also be removed in later section due to its rare frequency. Thus, the \"dr\" needs to be removed as well.\n",
        "\n",
        "Let's remove all of the tokens that has less than 3 letters."
      ],
      "metadata": {
        "id": "hK27LI-bIFmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# filtering tokens\n",
        "threshold = 3\n",
        "for key, value in gmap_id_token.items():\n",
        "    gmap_id_token[key] = [word for word in value if len(word) >= threshold]"
      ],
      "metadata": {
        "id": "Gn86hNNjJgqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check the number of tokens that we remove and also the tokens"
      ],
      "metadata": {
        "id": "m7JaVC32KA-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (Monash University, 2024)\n",
        "vocab = set(words)\n",
        "words_filtered = list(chain.from_iterable(gmap_id_token.values()))\n",
        "vocab_filtered = set(words_filtered)\n",
        "freq_dist_filtered = FreqDist(words_filtered)\n",
        "\n",
        "print(f\"The number of removed tokens : {len(list(vocab - set(freq_dist_filtered.keys())))}\")\n",
        "\n",
        "for i in range(0, len(list(vocab - set(freq_dist_filtered.keys()))), 10):\n",
        "    print(list(vocab - set(freq_dist_filtered.keys()))[i:i+10])"
      ],
      "metadata": {
        "id": "jX8Ooqj3KJgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see the sample output now."
      ],
      "metadata": {
        "id": "xaABoaWVNYzL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "for key, value in gmap_id_token.items():\n",
        "    print(f\"{key}: {value[:20]}\")\n",
        "    count += 1\n",
        "    if count == 5:\n",
        "        break"
      ],
      "metadata": {
        "id": "fKmCH7l3NeJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kW0WX96e17qG"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "    \n",
        "### 4.4. Stemming <a class=\"anchor\" name=\"stemming\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming is an essential step in text processing that involves reducing words to their root forms, thereby standardising variations of the same word. Looking at the sample results, we see many words that have similar meanings but are in different forms, such as \"feel,\" \"felt,\" and \"feeling,\" or \"doctor\" and \"doctors.\" Without stemming, these variations would be treated as distinct entities, which could dilute the analysis and lead to redundant or misleading insights. By applying stemming, we can consolidate these variations into a single representative form, such as \"feel\" and \"doctor,\" thus enhancing the consistency and clarity of the text data. This process helps improve the accuracy of tasks like sentiment analysis, keyword extraction, and text classification by ensuring that semantically similar words are grouped together.\n",
        "\n",
        "For this assignment, we are going to use stemming from PorterStemmer."
      ],
      "metadata": {
        "id": "D0-Xuv-0NipQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initiate the PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# looping each key to stem each tokens\n",
        "for key, value in gmap_id_token.items():\n",
        "    gmap_id_token[key] = [stemmer.stem(word) for word in value]"
      ],
      "metadata": {
        "id": "hECjcvk5OOpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see the result."
      ],
      "metadata": {
        "id": "Q85MkQAoPANy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's see the sample result\n",
        "for i in range(0, 200, 10):\n",
        "    print(gmap_id_token[\"0x14e4bcd95f3c0451:0x7ccf04478a4d59af\"][i:i+10])"
      ],
      "metadata": {
        "id": "Mh0crZ6XPCbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There might be tokens that have length less than 3 letters, however, we still want to keep that tokens because it might have meaning (due to stemming). But, we still need to check whether those tokens provide significant impact to the analysis later."
      ],
      "metadata": {
        "id": "cTOHF_AwS8ao"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bo_oS4YN2C8I"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "    \n",
        "### 4.5. Removing Context-Dependent Stopwords <a class=\"anchor\" name=\"context_dependent\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we are going to remove words that are very frequent throughout the business reviews but do not contribute significant meaning to the analysis. These context-dependent stopwords, though common within the dataset, do not help in distinguishing between different reviews or businesses. By filtering out these high-frequency words, we can reduce the noise in the data and enhance the focus on more meaningful terms that provide valuable insights into customer feedback and business performance.\n",
        "\n",
        "One issue with removing context-dependent stopwords after stemming is that it may lead to the removal of more tokens than if the stopwords were eliminated before stemming. This is because different words with distinct meanings can sometimes share the same root form. For example, words like \"manage\" and \"management\" may be reduced to the same stem, even though they convey different nuances. However, in most cases, these edge cases are minimal, and the risk of retaining context-dependent stopwords that fall below the frequency threshold due to slight variations in meaning is not worth it. Therefore, it's generally more effective to remove these stopwords after stemming to ensure that all irrelevant tokens are eliminated, thereby reducing noise and improving the quality of the data."
      ],
      "metadata": {
        "id": "I4FTH5gVP_EF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First let's get the frequency for distinct token per gmap_id."
      ],
      "metadata": {
        "id": "-dbyi-wXaZsU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get distinct token per gmap_id\n",
        "words_set = list(chain.from_iterable([set(value) for value in gmap_id_token.values()]))\n",
        "fd_set = FreqDist(words_set)\n",
        "\n",
        "for word in fd_set.most_common(10):\n",
        "    print(word)"
      ],
      "metadata": {
        "id": "yY2Nq2U5ZkMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's identify the context-dependent stopwords, which we define as those that appear in 95% of the gmap_id."
      ],
      "metadata": {
        "id": "TiYeR0BGaUbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 0.95 * len(gmap_id_token)\n",
        "context_dependent = [word for word, freq in fd_set.items() if freq > threshold]\n",
        "print(f\"The number of context dependent stopwords in the gmap_id: {len(context_dependent)}\")\n",
        "print(context_dependent)\n",
        "\n",
        "# remove the context dependent stopwords\n",
        "for key, value in gmap_id_token.items():\n",
        "    gmap_id_token[key] = [word for word in value if word not in context_dependent]"
      ],
      "metadata": {
        "id": "4kEklwGmahWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's reconfirm how many tokens are removed and how many unique tokens are removed."
      ],
      "metadata": {
        "id": "RFoSeKbEbGCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words_con_dependent = list(chain.from_iterable(gmap_id_token.values()))\n",
        "vocab_con_dependent = set(words_con_dependent)\n",
        "freq_dist_con_dependent = FreqDist(words_con_dependent)\n",
        "\n",
        "print(f\"The number of removed tokens : {len(words_filtered) - len(words_con_dependent)}\")\n",
        "print(f\"The number of unique removed tokens : {len(list(vocab_filtered - set(freq_dist_con_dependent.keys())))}\")\n",
        "\n",
        "# sample removed vocab\n",
        "for i in range(0, 100, 10):\n",
        "    print(list(vocab_filtered - set(freq_dist_con_dependent.keys()))[i:i+10])"
      ],
      "metadata": {
        "id": "rUOMBk53bljg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see the sample result"
      ],
      "metadata": {
        "id": "r8D_OAgE3ojr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "for key, value in gmap_id_token.items():\n",
        "    print(f\"{key}: {value[:10]}\")\n",
        "    count += 1\n",
        "    if count == 5:\n",
        "        break"
      ],
      "metadata": {
        "id": "q-OTDdfM3rVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_hUTjhX2DH9"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "    \n",
        "### 4.6. Removing Rare Tokens <a class=\"anchor\" name=\"rare_tokens\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we focus on removing rare tokens from the dataset. As discussed earlier, certain tokens, like doctor names such as “Lin,” add little value to the analysis and can introduce noise. By filtering out these infrequent tokens, we can enhance the quality of the data, keeping it focused on more impactful words.\n",
        "\n",
        "It’s more efficient to remove rare tokens after stemming, rather than before, to avoid redundant processing. Rare tokens, by definition, have minimal impact compared to context-dependent stopwords. Additionally, removing tokens prematurely might lead to the loss of potentially valuable counterparts later on. Therefore, it’s better to remove all rare tokens at once after stemming, ensuring that we retain as much useful information as possible.\n",
        "\n",
        "The rare tokens in this assignment is defined as tokens that appear in less than 5% of the whole token size."
      ],
      "metadata": {
        "id": "53BapQ_o15kY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First let's get the frequency for each tokens in the dictionary."
      ],
      "metadata": {
        "id": "xwRsoedXd6fp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 0.05 * len(gmap_id_token)\n",
        "\n",
        "rare_tokens = [word for word, freq in fd_set.items() if freq < threshold]\n",
        "print(f\"The number of rare tokens in the gmap_id: {len(rare_tokens)}\")\n",
        "\n",
        "# remove the context dependent stopwords\n",
        "for key, value in gmap_id_token.items():\n",
        "    gmap_id_token[key] = [word for word in value if word not in rare_tokens]"
      ],
      "metadata": {
        "id": "M4s093_XdfDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's reconfirm how many tokens are removed and how many unique tokens are removed."
      ],
      "metadata": {
        "id": "R9b0r7Vnhl6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words_rare = list(chain.from_iterable(gmap_id_token.values()))\n",
        "vocab_rare = set(words_rare)\n",
        "freq_dist_rare = FreqDist(words_rare)\n",
        "\n",
        "print(f\"The number of removed tokens : {len(words_con_dependent) - len(words_rare)}\")\n",
        "print(f\"The number of unique removed tokens :{len(list(vocab_con_dependent - set(freq_dist_rare.keys())))}\")\n",
        "print(f\"Check whether one of the doctor name 'Lin' is in the rare_vocab : {('lin' in rare_tokens)}\")\n",
        "\n",
        "for i in range(0, 20, 10):\n",
        "    print(list(vocab_con_dependent - set(freq_dist_rare.keys()))[i:i+10])"
      ],
      "metadata": {
        "id": "bwJNmJtyhl6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see sample result"
      ],
      "metadata": {
        "id": "kvnys78P4ZZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "for key, value in gmap_id_token.items():\n",
        "    print(f\"{key}: {value[:10]}\")\n",
        "    count += 1\n",
        "    if count == 5:\n",
        "        break"
      ],
      "metadata": {
        "id": "zh4t2Ddw4dK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbGHGJNZ2DPs"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "    \n",
        "### 4.7. Generating Bigrams <a class=\"anchor\" name=\"bigrams\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating bigrams is an essential step in text processing that allows us to capture pairs of words that often appear together, providing context that individual tokens might not fully convey. Bigrams can reveal important relationships between words that single-word tokens might miss, such as \"New York\" or \"artificial intelligence,\" which carry specific meanings when combined.\n",
        "\n",
        "It’s generally better to generate bigrams before performing data preprocessing steps like stopword removal. For example, a common stopword like \"at\" might be insignificant on its own, but in combination with other words like \"rate\", it forms meaningful phrases such as \"at rate,\" which we wouldn't want to lose by removing \"at\" too early in the process.\n",
        "\n",
        "In this section we will create the bigrams before the tokens are getting stemmed. This is due to that we want to preserves the tokens context. Stemming is changing tokens into the root words, thus it will make certain bigrams lose their context.\n",
        "\n",
        "As for frequency, it doesn’t pose a significant concern in this context, as we only select the top significant bigrams. This ensures that the most relevant and contextually important word pairs are retained, making our analysis more accurate and insightful.\n",
        "\n",
        "In this assignment we will be using PMI measurement and will take the top 200 bigrams tokens."
      ],
      "metadata": {
        "id": "nsGYhOhta43j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start with collecting all of the tokens from the original tokens dictionary, which we had created in section 4.1"
      ],
      "metadata": {
        "id": "1bANo8f0cK8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = list(chain.from_iterable(gmap_id_bigram.values()))"
      ],
      "metadata": {
        "id": "a9x7Ru0ZcuCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's use the nltk build in function which is BigramAssocMeasures"
      ],
      "metadata": {
        "id": "wSqZZbRhc_Fa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (Monash University, 2024)\n",
        "# initiate the BigramAssocMeasures\n",
        "token_bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
        "token_bigram_finder = nltk.collocations.BigramCollocationFinder.from_words(all_tokens)\n",
        "# do not include tokens length less than 3\n",
        "token_bigram_finder.apply_word_filter(lambda w: len(w) < 3)\n",
        "token_top_200_bigrams = token_bigram_finder.nbest(token_bigram_measures.pmi, 200)\n",
        "token_top_200_bigrams[:10]"
      ],
      "metadata": {
        "id": "1K_QWTGhdGjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's change the tokens associated with the token_top_200_bigrams in the gmap_id_bigrams into bigram using the MWETokenizer."
      ],
      "metadata": {
        "id": "it_Mj1rvr3-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gmap_mwetokenizer = MWETokenizer(token_top_200_bigrams)\n",
        "review_bigram_tokenized = dict((gmap_id_list, gmap_mwetokenizer.tokenize(review)) for gmap_id_list,review in gmap_id_bigram.items())\n",
        "\n",
        "all_word_collect = list(chain.from_iterable(review_bigram_tokenized.values()))\n",
        "total_vocab = list(set(all_word_collect))\n",
        "\n",
        "for i in range(0, 100, 10):\n",
        "    print(total_vocab[i:i+10])"
      ],
      "metadata": {
        "id": "DIOPa8iKd5JG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We do not need to stem the bigrams because the purpose of stemming is to group words with similar meanings together. However, in the context of bigrams, where the focus is on capturing meaningful phrases, stemming may not be as beneficial. Stemming could alter one or both words in a bigram, potentially distorting the phrase’s original meaning and reducing the accuracy of the analysis. For instance, stemming might convert “running late” to “run late,” which could change the nuance of the phrase. Therefore, it is more effective to keep bigrams intact without applying stemming, ensuring that the full context and meaning of the phrases are preserved."
      ],
      "metadata": {
        "id": "sjYyi1vWo7PA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to combine only the bigrams with the dictionary token that we have created in the previous sections.\n",
        "\n",
        "Also, let's check the number of bigrams and the total unique bigrams."
      ],
      "metadata": {
        "id": "OHc56S-2z46X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigrams_dict = {}\n",
        "\n",
        "for gmap_id, tokens in review_bigram_tokenized.items():\n",
        "    bigrams = [token for token in tokens if '_' in token]\n",
        "    bigrams_dict[gmap_id] = bigrams\n",
        "\n",
        "count = 0\n",
        "for key, bigrams in bigrams_dict.items():\n",
        "    print(f\"{key}: {bigrams[:10]}\")\n",
        "    count += 1\n",
        "    if count == 5:\n",
        "        break\n",
        "\n",
        "bigrams_word_collect = list(chain.from_iterable(bigrams_dict.values()))\n",
        "total_bigrams = list(set(bigrams_word_collect))\n",
        "\n",
        "print(f\"The total number of bigrams: {len(bigrams_word_collect)}\")\n",
        "print(f\"The total number of unique bigrams: {len(total_bigrams)}\")\n",
        "\n",
        "# append the tokens\n",
        "for key, bigrams in bigrams_dict.items():\n",
        "    gmap_id_token[key].extend(bigrams)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "mYK8v64atf4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check the result."
      ],
      "metadata": {
        "id": "Fn_5Rudu885U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words_final = list(chain.from_iterable(gmap_id_token.values()))\n",
        "vocab_final = set(words_final)\n",
        "freq_dist_final = FreqDist(words_final)\n",
        "\n",
        "bigram_words = list(chain.from_iterable(bigrams_dict.values()))\n",
        "bigram_vocab = set(bigram_words)\n",
        "bigram_freq_dist = FreqDist(bigram_words)\n",
        "\n",
        "print(f\"The total number of tokens before : {len(words_rare)}\")\n",
        "print(f\"The total number of tokens after : {len(words_final)}\")\n",
        "print(f\"The total number of bigram tokens that we add: {len(bigram_words)}\")\n",
        "print(f\"The total number of unique tokens before : {len(vocab_rare)}\")\n",
        "print(f\"The total number of unique tokens after : {len(vocab_final)}\")\n",
        "print(f\"The total number of unique bigram tokens that we add: {len(bigram_vocab)}\")"
      ],
      "metadata": {
        "id": "DJWkU2eZ9ANX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see sample result."
      ],
      "metadata": {
        "id": "mU8YlE6v-p0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "for key, value in gmap_id_token.items():\n",
        "    print(f\"{key}: {value[:10]}\")\n",
        "    count += 1\n",
        "    if count == 5:\n",
        "        break"
      ],
      "metadata": {
        "id": "eOqO2Z2t-vaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "Ve6IZ2I-lDEg"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "    \n",
        "### 4.8. Matrix Representation<a class=\"anchor\" name=\"Matrix\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erGhUY2UlDEg"
      },
      "source": [
        "One of the tasks is to generate the numerical representation for all tokens in abstract form, which involves converting the text data into a matrix format where each row represents a document and each column corresponds to a token or bigram. This matrix allows us to quantify the presence or frequency of each token in the dataset, enabling further analysis such as machine learning or clustering. The resulting matrix serves as a crucial input for various natural language processing (NLP) models, allowing them to process and learn from the data in a structured and efficient manner."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = set()\n",
        "for tokens in gmap_id_token.values():\n",
        "    all_tokens.update(tokens)\n",
        "\n",
        "all_tokens = sorted(all_tokens)\n",
        "\n",
        "matrix_representation = pd.DataFrame(0, index=gmap_id_token.keys(), columns=all_tokens)\n",
        "\n",
        "for key, tokens in gmap_id_token.items():\n",
        "    token_counts = defaultdict(int)\n",
        "    for token in tokens:\n",
        "        token_counts[token] += 1\n",
        "    for token, count in token_counts.items():\n",
        "        matrix_representation.loc[key, token] = count\n",
        "\n",
        "print(matrix_representation.shape)\n",
        "matrix_representation.head()"
      ],
      "metadata": {
        "id": "6PpZqfn6_wNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmaGJYIJlDEl"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "    \n",
        "## 5. Writing Output Files <a class=\"anchor\" name=\"write\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjMBqRetlDEl"
      },
      "source": [
        "files need to be generated:\n",
        "* Vocabulary list\n",
        "* Sparse matrix (count_vectors)\n",
        "\n",
        "This is performed in the following sections."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fc6tQ4ljlDEm"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "    \n",
        "### 5.1. Vocabulary List <a class=\"anchor\" name=\"write-vocab\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDlbpGYilDEm"
      },
      "source": [
        "List of vocabulary should also be written to a file, sorted alphabetically, with their reference codes in front of them. This file also refers to the sparse matrix in the next file. For this purpose, we will generate a vocabulary list where each token or bigram is assigned a unique reference code. These codes will be used to index the corresponding columns in the sparse matrix, allowing for efficient lookups and ensuring consistency between the vocabulary file and the matrix representation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6OUXHlxlDEm"
      },
      "outputs": [],
      "source": [
        "# make a new column called \"gmap_id\" to store the gmap_id\n",
        "matrix_representation.insert(0, 'gmap_id', matrix_representation.index)\n",
        "matrix_representation.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Create a dictionary that maps column names to indices\n",
        "column_index_mapping = {col: idx for idx, col in enumerate(matrix_representation.columns)}\n",
        "\n",
        "# Remove the 'gmap_id' entry from the dictionary\n",
        "column_index_mapping.pop('gmap_id', None)\n",
        "\n",
        "# Reassign the indices starting from 0\n",
        "column_index_mapping = {col: new_idx for new_idx, (col, _) in enumerate(column_index_mapping.items())}\n",
        "\n",
        "# Export the modified dictionary to a text file\n",
        "with open('/content/drive/MyDrive/Data Wrangling/assignment 1/Task 2/111_vocab.txt', 'w') as file:\n",
        "    for key, value in column_index_mapping.items():\n",
        "        file.write(f\"{key}: {value}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkGH81YFlDEn"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "    \n",
        "### 5.2. Sparse Matrix <a class=\"anchor\" name=\"write-sparseMat\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtxqUAwmlDEn"
      },
      "source": [
        "For writing the sparse matrix for a paper, we first calculate the frequency of words for that paper. Each word’s frequency is then represented in the matrix, with rows corresponding to individual documents (such as papers) and columns representing the unique tokens or bigrams from the vocabulary list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__n1fdIqlDEn"
      },
      "outputs": [],
      "source": [
        "# Open a text file for writing the output\n",
        "with open('/content/drive/MyDrive/Data Wrangling/assignment 1/Task 2/111_countvec.txt', 'w') as file:\n",
        "    # Iterate over each row in the DataFrame to create the desired output\n",
        "    for _, row in matrix_representation.iterrows():\n",
        "        gmap_id = row['gmap_id']  # Get the gmap_id\n",
        "        row_string = gmap_id + \",\"  # Start with the gmap_id\n",
        "\n",
        "        # Add index:value pairs for non-zero values\n",
        "        non_zero_values = [\n",
        "            f\"{column_index_mapping[col]}:{row[col]}\"\n",
        "            for col in matrix_representation.columns if row[col] != 0 and col != 'gmap_id'\n",
        "        ]\n",
        "\n",
        "        row_string += \",\".join(non_zero_values)\n",
        "\n",
        "        # Write the row string to the file\n",
        "        file.write(row_string + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUFQU-QXlDEn"
      },
      "source": [
        "-------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWjri6x_lDEn"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "    \n",
        "## 6. Summary <a class=\"anchor\" name=\"summary\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXAprlSblDEn"
      },
      "source": [
        "In this task, we systematically processed a dataset containing Google Map reviews from various businesses in California. The process began with loading and examining the input files, followed by a series of preprocessing steps to clean and prepare the data for further analysis. These steps included:\n",
        "\n",
        "1.\tTokenization: Breaking down the text into individual tokens, allowing us to work with manageable units of text.\n",
        "\n",
        "2.\tRemoving Context-Independent Stopwords: Eliminating common words like “the” and “is” that do not add significant meaning, thus reducing noise in the data.\n",
        "\n",
        "3.\tFiltering Tokens: Removing tokens that are less than three letters long, focusing on substantial words that contribute meaning.\n",
        "\n",
        "4.\tStemming: Reducing words to their base form (e.g., “running” becomes “run”) to group similar words together, ensuring consistency in the analysis.\n",
        "\n",
        "5.\tRemoving Context-Dependent Stopwords: Filtering out words that, although frequent across the dataset, do not distinguish between different reviews, such as “great” or “lot.”\n",
        "\n",
        "6.\tRemoving Rare Tokens: Excluding infrequent tokens that are unlikely to add significant insights, thereby reducing noise in the dataset.\n",
        "\n",
        "7.\tGenerating Bigrams: Creating pairs of words (e.g., “New York”) that frequently appear together to capture meaningful phrases that might be missed if words were only considered individually.\n",
        "\n",
        "8.\tMatrix Representation: Converting the processed tokens into a numerical matrix format, where each row represents a document and each column corresponds to a token or bigram, enabling further analysis like machine learning or clustering.\n",
        "\n",
        "The final output included a vocabulary list and a sparse matrix, both of which were exported to files for future use in analytical tasks. These preprocessing steps ensured that the dataset was thoroughly cleaned, organized, and ready for advanced processing, ultimately enabling more accurate and insightful analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFXYKxO8lDEn"
      },
      "source": [
        "-------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HppxDtWNlDEn"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "    \n",
        "## 7. References <a class=\"anchor\" name=\"Ref\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCkWr-M1lDEo"
      },
      "source": [
        "[1] Pandas dataframe.drop_duplicates(), https://www.geeksforgeeks.org/python-pandas-dataframe-drop_duplicates/, Accessed 13/08/2022.\n",
        "[2] Monash University. (2024). Week 5 Applied Session: Text Pre-Processing [Jupyter Notebook]. Monash University, FIT5196. Accessed August 24, 2024, from https://colab.research.google.com/drive/1OSc-vgcp3rZv9dRUjXLEUCgejbmQpFxo?usp=sharing\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mp9O-a1UlDEo"
      },
      "source": [
        "## --------------------------------------------------------------------------------------------------------------------------"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "z8mo6PPRlDEU",
        "bSr_kwKclDEV",
        "SA7fSJiRlDEY",
        "Ve6IZ2I-lDEg",
        "Fc6tQ4ljlDEm",
        "YkGH81YFlDEn",
        "AWjri6x_lDEn",
        "HppxDtWNlDEn"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}